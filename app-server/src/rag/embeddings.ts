import { Embeddings, EmbeddingsParams } from '@langchain/core/embeddings';
import { pipeline, FeatureExtractionPipeline } from '@huggingface/transformers';

/**
 * @huggingface/transformers (v3)ë¥¼ ì‚¬ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ ì„ë² ë”© í´ë˜ìŠ¤
 * LangChainì˜ Embeddings ì¶”ìƒ í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ í˜¸í™˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
 */
export class LocalHuggingFaceEmbeddings extends Embeddings {
  private model: string;
  private pipe: FeatureExtractionPipeline | null = null;

  constructor(fields?: EmbeddingsParams & { model?: string }) {
    super(fields ?? {});
    // ê¸°ë³¸ ëª¨ë¸: ì‘ê³  ë¹ ë¥´ë©° ì„±ëŠ¥ì´ ê²€ì¦ëœ all-MiniLM-L6-v2 ì‚¬ìš©
    this.model = fields?.model ?? 'Xenova/all-MiniLM-L6-v2';
  }

  /**
   * íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œí•©ë‹ˆë‹¤. (Lazy Loading)
   */
  async ensurePipeline() {
    if (!this.pipe) {
      console.log(`ğŸ”§ Initializing local embedding pipeline: ${this.model}`);
      const pipe = await pipeline('feature-extraction', this.model);
      this.pipe = pipe;
    }
  }

  /**
   * ë¬¸ì„œ ë°°ì—´(Documents)ì„ ë²¡í„° ë°°ì—´ë¡œ ë³€í™˜
   */
  async embedDocuments(documents: string[]): Promise<number[][]> {
    await this.ensurePipeline();
    const embeddings: number[][] = [];

    for (const doc of documents) {
      // pooling: 'mean' -> ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê· ì„ êµ¬í•´ ë¬¸ì¥ ì „ì²´ ë²¡í„° ìƒì„±
      // normalize: true -> ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ ì •ê·œí™”
      const output = await this.pipe!(doc, {
        pooling: 'mean',
        normalize: true,
      });

      // Tensor ë°ì´í„°ë¥¼ ì¼ë°˜ ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
      embeddings.push(Array.from(output.data));
    }
    return embeddings;
  }

  /**
   * ê²€ìƒ‰ì–´(Query)ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
   */
  async embedQuery(document: string): Promise<number[]> {
    await this.ensurePipeline();
    const output = await this.pipe!(document, {
      pooling: 'mean',
      normalize: true,
    });
    return Array.from(output.data);
  }
}
