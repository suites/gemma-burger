## Fine-tuning

resources/fine_tuning ν΄λ”μ ν•™μµμ© λ°μ΄ν„°μ…‹κ³Ό κ²€μ¦μ© λ°μ΄ν„°μ…‹μΌλ΅ ν•™μµμ„ μ§„ν–‰ν•©λ‹λ‹¤.

### λ°μ΄ν„° μ¤€λΉ„

Hugging Faceμ Chat Templates ν•νƒλ΅ λ°μ΄ν„°λ¥Ό μ¤€λΉ„ν•©λ‹λ‹¤.
https://huggingface.co/docs/transformers/chat_templating

```json
{
  "messages": [
    { "role": "user", "content": "Hi there!" },
    {
      "role": "assistant",
      "content": "Hello! Welcome to Gemma Burger! π” How can I help you today? π‹"
    }
  ]
}
```

- LoRA (Low-Rank Adaptation)
  - LLMμ„ ν¨μ¨μ μΌλ΅ ν•™μµμ‹ν‚¤κΈ° μ„ν• κ²½λ‰ν™” νμΈνλ‹ κΈ°λ²•
  - κΈ°μ΅΄μ νμΈνλ‹μ€ λ¨λΈμ λ¨λ“  νλΌλ―Έν„° (Gemma 2Bμ κ²½μ° μ•½ 26μ–µκ°)λ¥Ό μ „λ¶€ μ—…λ°μ΄νΈ ν–μµλ‹λ‹¤.
    - VRAMμ΄ μ—„μ²­λ‚κ² ν•„μ”ν•κ³  ν•™μµμ‹κ°„μ΄ μ¤λ κ±Έλ¦½λ‹λ‹¤.
  - LoRAλ” λ‹¤μκ³Ό κ°™μ€ νΉμ§•μ„ κ°€μ§€κ³  μμµλ‹λ‹¤.
    - LLMμ νλΌλ―Έν„°λ¥Ό Freezeν•©λ‹λ‹¤.
    - μ†μ— μ‘μ€ ν–‰λ ¬μ„ λ¶™μ—¬μ„ ν•™μµν•©λ‹λ‹¤.
    - λ¨λΈμ μΌλ¶€ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•μ—¬ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ μ¤„μ…λ‹λ‹¤.
    - ν•™μµ μ‹κ°„μ„ λ‹¨μ¶•ν•  μ μμµλ‹λ‹¤.

$$W_{new} = W_{old} + \Delta W = W_{old} + (A \times B)$$

- $W_{old}$: μ›λ λ¨λΈμ κ°€μ¤‘μΉ ($d \times d$ ν–‰λ ¬, κ³ μ •λ¨)
- $A, B$: ν•™μµ κ°€λ¥ν• μ‘μ€ ν–‰λ ¬λ“¤ ($d \times r$, $r \times d$)
- $r$ (Rank): μ°λ¦¬κ°€ μ„¤μ • νμΌ(lora_config.yaml)μ—μ„ **rank: 8**λ΅ μ„¤μ •ν• κ°’μ…λ‹λ‹¤. μ΄ μ«μκ°€ μ‘μ„μλ΅ ν•™μµν•  μ–‘μ΄ μ¤„μ–΄λ“­λ‹λ‹¤.

### Train

```bash
poetry run mlx_lm.lora --config lora_config.yaml --train
```

ν•™μµμ΄ λλ‚λ©΄ λ°μ΄ν„°λ” adapters ν΄λ”μ— μ €μ¥λ©λ‹λ‹¤.

```bash
|
β””β”€β”€ adapters
    β””β”€β”€ 0000600_adapters.safetensors
    β””β”€β”€ adapter_config.json
    β”β”€β”€ adapters.safetensors
```
