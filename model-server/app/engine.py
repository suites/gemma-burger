from mlx_lm import generate, load
from mlx_lm.sample_utils import make_sampler

# ì‚¬ìš©í•  ëª¨ë¸ ID (Hugging Face Hub ê¸°ì¤€)
# 4bit ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
MODEL_ID = "mlx-community/gemma-3-4b-it-4bit"  # í˜¹ì€ "mlx-community/gemma-3-4b-it-4bit" ë“±ì„ ì‚¬ìš© ê°€ëŠ¥


class LLMEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        print(f"ğŸš€ Loading model: {MODEL_ID}...")

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œë¨)
        # tokenizer_config={"trust_remote_code": True}ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
        self.model, self.tokenizer = load(MODEL_ID)
        print("âœ… Model loaded successfully!")

    def generate_text(
        self, prompt: str, max_tokens: int = 200, temperature: float = 0.7
    ) -> str:
        if not self.model:
            raise RuntimeError("Model is not loaded!")

        messages = [{"role": "user", "content": prompt}]

        # ì˜ˆ: "hello" -> "<start_of_turn>user\nhello<end_of_turn>\n<start_of_turn>model\n"
        prompt_formatted = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        response = generate(
            self.model,
            self.tokenizer,
            prompt=prompt_formatted,
            max_tokens=max_tokens,
            sampler=make_sampler(temp=temperature),
            verbose=True,
        )
        return response


# ì‹±ê¸€í†¤ íŒ¨í„´ì²˜ëŸ¼ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë¡œ ê´€ë¦¬ (FastAPI ì‹œì‘ ì‹œ ë¡œë“œ)
engine = LLMEngine()
