import os

import mlx.core as mx
from mlx_lm import load, stream_generate
from mlx_lm.sample_utils import make_sampler

mx.set_default_device(mx.gpu)

MODEL_ID = "mlx-community/gemma-3-4b-it-4bit"


class LLMEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        print(f"ğŸš€ Loading model: {MODEL_ID}...")

        adapter_path = "adapters"

        if os.path.exists(adapter_path):
            print(f"âœ¨ Found adapter at '{adapter_path}'. Loading with LoRA...")
            self.model, self.tokenizer = load(MODEL_ID, adapter_path=adapter_path)
        else:
            print("âš ï¸ Adapter not found. Loading base model only.")
            self.model, self.tokenizer = load(MODEL_ID)

        print("âœ… Model loaded successfully!")

    def generate_text_stream(
        self, prompt: str, max_tokens: int = 200, temperature: float = 0.7
    ):
        """
        í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜
        """
        if not self.model:
            raise RuntimeError("Model is not loaded!")

        messages = [{"role": "user", "content": prompt}]
        prompt_formatted = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # stream_generate í•¨ìˆ˜ ì‚¬ìš©
        # ì´ í•¨ìˆ˜ëŠ” (token, text) íŠœí”Œì„ yield í•©ë‹ˆë‹¤.
        stream = stream_generate(
            self.model,
            self.tokenizer,
            prompt=prompt_formatted,
            max_tokens=max_tokens,
            sampler=make_sampler(temp=temperature),
        )

        for response in stream:
            # response.textì— ìƒˆë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
            # ì´ê²ƒì„ ë°”ë¡œë°”ë¡œ yield í•˜ì—¬ í˜¸ì¶œìì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
            yield response.text


engine = LLMEngine()
