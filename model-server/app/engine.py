# model-server/app/engine.py

from mlx_lm import generate, load

# ì‚¬ìš©í•  ëª¨ë¸ ID (Hugging Face Hub ê¸°ì¤€)
# 4bit ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
MODEL_ID = (
    "google/gemma-3-4b-it"  # í˜¹ì€ "mlx-community/gemma-3-4b-it-4bit" ë“±ì„ ì‚¬ìš© ê°€ëŠ¥
)


class LLMEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        print(f"ğŸš€ Loading model: {MODEL_ID}...")

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œë¨)
        # tokenizer_config={"trust_remote_code": True}ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
        self.model, self.tokenizer = load(MODEL_ID)
        print("âœ… Model loaded successfully!")

    def generate_text(
        self, prompt: str, max_tokens: int = 200, temp: float = 0.7
    ) -> str:
        """
        ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if not self.model:
            raise RuntimeError("Model is not loaded!")

        # MLX-LMì˜ generate í•¨ìˆ˜ëŠ” ë§¤ìš° ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        response = generate(
            self.model,
            self.tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temp=temp,
            verbose=True,  # í„°ë¯¸ë„ì— ìƒì„± ê³¼ì •ì„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        )
        return response


# ì‹±ê¸€í†¤ íŒ¨í„´ì²˜ëŸ¼ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë¡œ ê´€ë¦¬ (FastAPI ì‹œì‘ ì‹œ ë¡œë“œ)
engine = LLMEngine()
