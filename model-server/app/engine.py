import os

import mlx.core as mx
from mlx_lm import load, stream_generate, generate
from mlx_lm.sample_utils import make_sampler

mx.set_default_device(mx.gpu)

MODEL_ID = "mlx-community/gemma-3-4b-it-4bit"


class LLMEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        print(f"ğŸš€ Loading model: {MODEL_ID}...")

        adapter_path = "adapters"

        if os.path.exists(adapter_path):
            print(f"âœ¨ Found adapter at '{adapter_path}'. Loading with LoRA...")
            self.model, self.tokenizer = load(MODEL_ID, adapter_path=adapter_path)
        else:
            print("âš ï¸ Adapter not found. Loading base model only.")
            self.model, self.tokenizer = load(MODEL_ID)

        print("âœ… Model loaded successfully!")

    def generate_text_stream(
        self, prompt: str, max_tokens: int = 200, temperature: float = 0.7
    ):
        """
        í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜
        """
        if not self.model:
            raise RuntimeError("Model is not loaded!")

        messages = [{"role": "user", "content": prompt}]
        prompt_formatted = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # stream_generate í•¨ìˆ˜ ì‚¬ìš©
        # ì´ í•¨ìˆ˜ëŠ” (token, text) íŠœí”Œì„ yield í•©ë‹ˆë‹¤.
        stream = stream_generate(
            self.model,
            self.tokenizer,
            prompt=prompt_formatted,
            max_tokens=max_tokens,
            sampler=make_sampler(temp=temperature),
        )

        for response in stream:
            # response.textì— ìƒˆë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
            # ì´ê²ƒì„ ë°”ë¡œë°”ë¡œ yield í•˜ì—¬ í˜¸ì¶œìì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
            yield response.text

    def generate_text(
        self, prompt: str, max_tokens: int = 100, temperature: float = 0.0
    ) -> str:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ í•œ ë²ˆì— í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì£¼ë¡œ ë‚´ë¶€ ë¡œì§(Intent ë¶„ë¥˜, Tool í˜¸ì¶œ ë“±)ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        if not self.model:
            raise RuntimeError("Model is not loaded!")

        # Routerìš© í”„ë¡¬í”„íŠ¸ëŠ” ë³´í†µ ì´ë¯¸ ì™„ì„±ëœ í˜•íƒœ(System Prompt í¬í•¨)ë¡œ ë“¤ì–´ì˜¤ì§€ë§Œ,
        # Chat Templateì„ ì ìš©í•´ì•¼ ëª¨ë¸ì´ ë” ì˜ ì•Œì•„ë“£ìŠµë‹ˆë‹¤.
        # (ë‹¨, ì…ë ¥ promptê°€ ì´ë¯¸ í¬ë§·íŒ…ëœ ìƒíƒœë¼ë©´ ì´ ê³¼ì •ì€ ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        #  ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ 'user' ë©”ì‹œì§€ë¡œ ê°ì‹¸ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.)
        messages = [{"role": "user", "content": prompt}]
        prompt_formatted = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        response = generate(
            self.model,
            self.tokenizer,
            prompt=prompt_formatted,
            max_tokens=max_tokens,
            sampler=make_sampler(temp=temperature), # ë¶„ë¥˜ ì‘ì—…ì€ ì°½ì˜ì„±ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ temp=0.0 ê¶Œì¥
            verbose=False # ë¡œê·¸ ì§€ì €ë¶„í•´ì§€ì§€ ì•Šê²Œ ë”
        )
        
        return response

engine = LLMEngine()
