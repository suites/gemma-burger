from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from app.engine import engine
from app.agent import agent_app # â¬…ï¸ ì—ì´ì „íŠ¸ ì•± import

app = FastAPI(title="Gemma Agent Server")

class ChatRequest(BaseModel):
    message: str

@app.post("/chat")
def chat_endpoint(req: ChatRequest):
    try:
        print(f"ğŸ“© User Query: {req.message}")

        # 1. LangGraph ì—ì´ì „íŠ¸ ì‹¤í–‰
        # ì—ì´ì „íŠ¸ê°€ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ 'í”„ë¡¬í”„íŠ¸'ë¥¼ ê²°ì •í•´ì¤ë‹ˆë‹¤.
        initial_state = {
            "messages": [{"role": "user", "content": req.message}],
            "current_intent": "general",
            "order_items": [],
            "final_response": ""
        }
        
        # invoke()ë¥¼ ì‹¤í–‰í•˜ë©´ ê·¸ë˜í”„ë¥¼ íƒ€ê³  ëê¹Œì§€ ê°€ì„œ ê²°ê³¼ë¥¼ ì¤ë‹ˆë‹¤.
        result = agent_app.invoke(initial_state)
        
        # ì—ì´ì „íŠ¸ê°€ ê²°ì •í•œ ìµœì¢… í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        final_prompt = result["final_response"]
        
        print(f"ğŸ¤– Agent decided prompt: {final_prompt[:50]}...")

        # 2. [Generate] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ê¸°ì¡´ ì—”ì§„ ì‚¬ìš©)
        return StreamingResponse(
            engine.generate_text_stream(
                prompt=final_prompt,
                max_tokens=500,
                temperature=0.7
            ),
            media_type="text/plain"
        )

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)